---
sidebar_position: 3
---

## Sensor Simulation

A robot is blind, deaf, and senseless without its sensors. A digital twin is no different. One of the most powerful features of Gazebo is its ability to simulate a wide variety of sensors, allowing you to develop and test perception and control algorithms on realistic, synthesized data.

These simulated sensors are added to your robot model (typically in the URDF or SDF file) and are attached to specific links. Gazebo uses plugins to generate sensor data and publish it over ROS 2 topics, just as a real sensor driver would. This means your ROS 2 nodes can subscribe to `/lidar_scan` or `/camera/image_raw` without needing to know whether the data is coming from a real robot or a simulated one.

Let's explore some of the most common sensors you will simulate for a humanoid robot.

### LiDAR Simulation

**LiDAR (Light Detection and Ranging)** sensors are crucial for navigation and obstacle avoidance. They work by emitting laser beams and measuring the time it takes for them to reflect off objects, thus calculating distance.

In Gazebo, a simulated LiDAR sensor performs ray-casting within its field of view. It casts out a specified number of virtual laser beams and reports the distance to the first object each ray intersects.

You can configure many parameters, including:
*   **Scan range**: The minimum and maximum distance the sensor can measure.
*   **Field of View (FoV)**: The angle the sensor sweeps.
*   **Number of rays**: The angular resolution of the scan.
*   **Update rate**: How many scans per second are published.
*   **Noise**: You can add Gaussian noise to the measurements to make the simulation more realistic, mimicking the imperfections of a real-world sensor.

The data is typically published as a `sensor_msgs/msg/LaserScan` message on a ROS 2 topic.

### Depth Camera Simulation

A **depth camera** is another critical sensor for 3D perception. Like a regular RGB camera, it captures an image, but instead of color, each pixel in the image represents the distance from the camera to that point in the scene.

Gazebo simulates a depth camera by using the GPU's depth buffer, which is a standard feature in 3D graphics rendering. It renders the scene from the camera's perspective and captures the distance information for each pixel.

Common configurations include:
*   **Image resolution**: The width and height of the depth image in pixels.
*   **FoV**: The camera's field of view.
*   **Clipping planes**: The minimum and maximum distances the camera can perceive.
*   **Distortion models**: To simulate real-world lens distortion.

This data is published as a `sensor_msgs/msg/Image` (where the pixel encoding is for depth, e.g., `32FC1`) or a `sensor_msgs/msg/PointCloud2` message, which represents the scene as a 3D cloud of points.

### IMU & Force-Torque Sensors

These sensors are vital for a humanoid's stability and interaction with the environment.

*   **IMU (Inertial Measurement Unit)**: An IMU measures a body's orientation, angular velocity, and linear acceleration. It's the key to balance. In Gazebo, the IMU plugin directly accesses the physics state (position, velocity, acceleration) of the link it's attached to. It then adds configurable noise and bias to simulate the drift and inaccuracies of a real IMU. The data is published as a `sensor_msgs/msg/Imu` message.

*   **Force-Torque (F/T) Sensors**: These sensors are typically placed in a robot's wrists or ankles to measure the forces and torques applied during interaction. They are essential for tasks like compliant manipulation or maintaining stable foot contact. Gazebo's F/T sensor plugin reports the wrench (a combination of force and torque) acting on the joint it's associated with. This data is published as a `geometry_msgs/msg/WrenchStamped` message.

### Data Topic Examples

When you launch a Gazebo simulation with a robot equipped with these sensors, you can expect to see ROS 2 topics like these (the exact names can be configured):

| Sensor          | ROS 2 Topic Name             | Message Type                    | Description                                       |
| --------------- | ---------------------------- | ------------------------------- | ------------------------------------------------- |
| **LiDAR**       | `/scan` or `/lidar_scan`     | `sensor_msgs/msg/LaserScan`     | An array of distance measurements from the laser. |
| **Depth Camera**| `/camera/depth/image_raw`    | `sensor_msgs/msg/Image`         | A depth image.                                    |
| **Depth Camera**| `/camera/points`             | `sensor_msgs/msg/PointCloud2`   | A 3D point cloud of the scene.                    |
| **IMU**         | `/imu/data`                  | `sensor_msgs/msg/Imu`           | Orientation, angular velocity, and acceleration.|
| **F/T Sensor**  | `/wrist_ft_sensor/wrench`    | `geometry_msgs/msg/WrenchStamped` | Forces and torques on the robot's wrist.          |

By using these standard message types, the same software nodes that process data from a real robot can be used, without modification, on the digital twin. This seamless transition between simulation and reality is a cornerstone of modern robotics development.
